{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Dask\n",
    "\n",
    "Dask is a flexible parallel computing library for Python. It allows dynamic scheduling of tasks for optimising computations and includes collections (extensions of arrays, lists and dataframes) for use with parallel data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "#### Thread\n",
    "A thread is the simplest unit of computation.\n",
    "\n",
    "#### Process\n",
    "A process is an isolated computation which consists of one or more threads. The threads within a process can be executed concurrently (simultaneously) and have access to the same resources within the process (memory, executable code and variable values). Processes cannot share these resources with other processes. Each process has its own individual address space, while the threads within that process share their address space.\n",
    "\n",
    "#### Graph\n",
    "A way to represent 'things' and their relationships, where a node (circle) is a thing and an edge (line) between nodes is their relationship; e.g. a social network where nodes are people and edges are their friendships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask\n",
    "\n",
    "#### Dask architecture\n",
    "Below is the basic structure of how the components of Dask interact:\n",
    "\n",
    "![](../images/dask_architecture.png)\n",
    "\n",
    "Dask uses a pool of workers to process tasks specified by the scheduler. The scheduler determines the tasks to be done by intelligently traversing the task graph. The task graph is produced dynamically and automatically passed to the scheduler when using dask functionality in your code. In the case of dask distributed, the task graph is submitted to the scheduler by the client.\n",
    "\n",
    "#### Scheduler\n",
    "One of Dask’s key benefits is its ability to efficiently schedule tasks to optimize computations. Everything Dask does is built on top of “schedulers”. These schedulers take the order of work established by a task graph and find the optimal way to break down and carry out the tasks.\n",
    "\n",
    "Dask has four types of schedulers:\n",
    "- Synchronous: Single thread (good for debugging).\n",
    "        dask.get\n",
    "- Threaded: Utilises a thread pool.\n",
    "        dask.threaded.get\n",
    "- Multiprocessing: Utilises a process pool.\n",
    "        dask.multiprocessing.get\n",
    "- Distributed: Utilises a cluster of distributed machines.\n",
    "        distributed.Client.get\n",
    "\n",
    "#### Worker\n",
    "A worker receives tasks to process from the scheduler and returns output to the scheduler when that processing is finished. A worker can be a thread, a process or a whole machine, depending on the scheduler used.\n",
    "\n",
    "#### Client\n",
    "A client provides the primary point of access to a distributed scheduler and its associated workers. When using a threaded or multiprocessing scheduler interation with the scheduler is handled by the collection or delayed object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Graph\n",
    "A graph of tasks (nodes) and the data which is required to pass between them (edges).\n",
    "In the context of dask:\n",
    "- Task = circle\n",
    "- Data = box\n",
    "- Direction of flow = arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "x = 1\n",
    "y = 2\n",
    "z = add(x, y)\n",
    "w = sum([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/task_graph_def_img_1.png)\n",
    "\n",
    "Viewing the relationship between tasks and data can reveal better\\* ways to order the tasks to achieve the same computation.\n",
    "\n",
    "\\* Better can mean in less time or using less memory or both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask Graph\n",
    "Dask stores task graphs in a Python dictionary which maps keys to computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsk = {'x': 1,\n",
    "       'y': 2,\n",
    "       'z': (add, 'x', 'y'),\n",
    "       'w': (sum, ['x', 'y', 'z'])\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is different from other parallelising libraries in that it uses ordinary Python structures to represent task graphs instead of a specialised API:\n",
    "\n",
    "- `{dicts}`\n",
    "- `(tuples)`\n",
    "- `functions()`\n",
    "- `values`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Terminology\n",
    "\n",
    "Now it is time to test what you have learned! Answer the following questions by assigning each a definition.\n",
    "\n",
    "Definitions:\n",
    "1. A group of threads with shared memory.\n",
    "2. The object which coordinates tasks for workers to process.\n",
    "3. An interface to access the scheduler in a distrubuted parallel computing cluster.\n",
    "4. A representation of the relationsips between a set of interdependent tasks and their data.\n",
    "5. The smallest unit of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Q: What is a scheduler? (Answer in the cell below.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Q: What is a thread?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is a task graph?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is a process?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is a client?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Delayed\n",
    "As users we may want to use the functionality of Dask to parallelise our own code. The `dask.delayed` interface allows us to do this with a very light API:\n",
    "\n",
    "![](../images/dask_stack.png)\n",
    "\n",
    "Here we see that all of Dask is built on top of the `scheduler`. The graph spec layer allows graphs to be generated and interpreted by the scheduler. Dask collections such as `arrays`, `bags` and `dataframes` are built on top of these and provide functionality to use these data types without having to think about how to talk to the `scheduler` underneath. Similarly `delayed` provides us with a lightweight interface to create our own parallelisable code without need to directly talk to the scheduler.\n",
    "\n",
    "The decorator `@delayed` is used to \"dask-ify\" any arbitrary function, which allows said function to be used as part of a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "@delayed(pure=True)\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "@delayed(pure=True)\n",
    "def mul(a, b):\n",
    "    return a * b\n",
    "\n",
    "@delayed(pure=True)\n",
    "def inc(a):\n",
    "    return a + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the cells below you will see `z` is a delayed object and the dask methods `visualize()` and `compute()` can be used on them like any other dask collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = add(1, 2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly `c` is a delayed object which can be visualized and computed; it is slightly more complex than `z` on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = inc(1)\n",
    "b = mul(1, 2)\n",
    "c = add(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that here we have created objects which are \"lazy\"; when they are created they are not immediately executed. Their execution is `delayed` until a time which we the users (or a scheduler) determine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: 1\n",
    "Run the code in the cell below and then visualize and compute `total`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for x in range(4):\n",
    "    a = inc(1)\n",
    "    b = mul(1, x)\n",
    "    c = add(a, b)\n",
    "    results.append(c)\n",
    "\n",
    "total = delayed(sum, pure=True)(results)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** Visualize total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2:** Compute total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Iris Load with Dask\n",
    "Iris is a Python library for analysing and visualising meteorological and oceanographic data sets. A common issue when using Iris to analyse many data files is that loading these files can take a lot of time when done sequentially. A solution is to parallelise the loading of files, which is an ideal task for Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will need a few more libraries to load files using Iris. These include `iris` for loading files to cubes, `os` and `glob` to access files on disk, and `dask.bag` is the type of dask collection we will use to create a task graph of our parallelised load functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "import os\n",
    "import glob\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be loading is monthly northward sea ice velocities from 1890-1899, generated by the Met Office Unified Model and stored in 120 PP files. We will create a list of the filenames, and check the number and names of the files in the list are what we expect, using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(iris.sample_data_path('UM', '*.pp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Earliest: {}\\nLatest: {}'.format(files[0], files[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into multiple cubes using `iris.load_cube`\n",
    "\n",
    "When using Iris to load data that data is stored in a structure called a `cube`. Multiple cubes can be stored in a list called a `CubeList`. We can load the data from our 120 PP files into a `CubeList` by mapping our list of filenames (`files`) onto the `iris.load_cube()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cubelist = iris.cube.CubeList(map(iris.load_cube, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(cubelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(cubelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into multiple cubes using `dask.bag`\n",
    "\n",
    "We can parallelise the loading of data from files into cubes by creating a `dask.bag` object which maps our list of filenames onto the function `iris.load_cube()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_o_cubes = db.from_sequence(files).map(iris.load_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(bag_o_cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_o_cubes.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bag_o_cubes` is a delayed (or \"lazy\") object which we can `visualize` as a task graph. The delayed object will not perform the computation of mapping filenames onto `iris.load_cube()` until we give it the command to do so (namely `bag_o_cubes.compute()`). You can see from the task graph (double-click to enlarge) that each file is being loaded into a cube invidually and in parallel.\n",
    "\n",
    "Since we want to generate a `CubeList` like the previous example we can compute `bag_o_cubes` from within our `iris.cube.CubeList()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_cubelist = iris.cube.CubeList(bag_o_cubes.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(bag_cubelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(bag_cubelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that part of the beauty of Python is that we do not necessarily need to create four objects (`fp`, `files`,  `bag_o_cubes`, `bag_cubelist`) to get a `CubeList` object but can chain the functions together into one big function call:\n",
    "\n",
    "```python\n",
    "iris.cube.CubeList(db.from_sequence(files).map(iris.load_cube).compute())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into single cube using `iris.load`\n",
    "\n",
    "Iris by default will try to merge all input files into the fewest resultant cubes as possible, obeying the rules on what data and metadata can be merged into a single cube. If we know that the result of a load operation will be a single cube we can use `iris.load_cube` to return a single merged cube from the load operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cube = iris.load_cube(files)\n",
    "\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the output of printing a single cube and it is clear that the decade's worth of data has been merged into a single cube with 120 time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_cube = iris.load_cube(files[0])\n",
    "\n",
    "print(single_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into single cube using `dask.bag` and `dask.delayed`\n",
    "We can merge the `CubeList` we generate using a `dask.bag` in exactly the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_merged_cube = bag_cubelist.merge_cube()\n",
    "\n",
    "print(bag_merged_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the `merge_cube` step in this does not leverage Dask. We can utilise Dask in the merge step by decorating a call to `merge_cube` with the `@delayed` decorator. This allows us to integrate it with our task graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@delayed\n",
    "def delayed_merge(cubes):\n",
    "    return iris.cube.CubeList(cubes).merge_cube()\n",
    "\n",
    "delayed_merged_cube = delayed_merge(bag_o_cubes)\n",
    "\n",
    "delayed_merged_cube.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again if you expand the task graph (double-click image) you can see that `merge_cube()` is now part of it. Compare this to the task graph of `bag-o-cubes`. What is different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`delayed_merged_cube` is \"lazy\" so we need to call `compute()` to actually execute the task graph and output a merged cube. We can do this within a `print()` call to see that we have merged it successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(delayed_merged_cube.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Loading\n",
    "\n",
    "Consider the following code snippets. Despite looking similar, not all of these code snippets will provide parallel file loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell sets up the files needed by the exercise.\n",
    "filepath = iris.sample_data_path('GloSea4')\n",
    "files = glob.glob(filepath)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dlyd = delayed(iris.load_cube)(os.path.join(filepath, '*.pp'))\n",
    "cs1 = db.from_delayed(dlyd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs2 = db.from_sequence(files).map(iris.load_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs3 = db.from_delayed(map(delayed(iris.load_cube), files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Print the task graph for each code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** With reference to the task graph produced by each code snippet, state whether or not each code snippet will provide parallel file loading."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** For the code snippets that do not provide parallel file loading, describe why parallel file loading does not happen and suggest a change to the code snippet to enable parallel file loading."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
